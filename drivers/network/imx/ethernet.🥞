#define IRQ_CH 0
#define TX_CH  1
#define RX_CH  2

#define RX_COUNT 256
#define TX_COUNT 256

#define IRQ_MASK_OFFSET WORD_SIZE
#define RX_QUEUE_OFFSET WORD_SIZE * 3
#define TX_QUEUE_OFFSET WORD_SIZE * 4
#define HW_RX_RING_OFFSET WORD_SIZE * 5
#define HW_TX_RING_OFFSET WORD_SIZE * 6

#define get_rx_q_handle(rx_q_handle) \
    var rx_q_handle = lds {1} @base + RX_QUEUE_OFFSET \

#define get_tx_q_handle(tx_q_handle) \
    var tx_q_handle = lds {1} @base + TX_QUEUE_OFFSET \

#define get_hw_rx(hw_rx) \
    var hw_rx = lds {1} @base + HW_RX_RING_OFFSET \

#define get_hw_tx(hw_tx) \
    var hw_tx = lds {1} @base + HW_TX_RING_OFFSET \

#define TMP_STORE @base + WORD_SIZE * 42
#define TMP_LARGE @base + WORD_SIZE * 43

/* IRQ */
#define get_irq_mask(irq) \
    var irq_addr = lds {1} @base + IRQ_MASK_OFFSET \
    var irq = !ldw irq_addr \

#define enable_irqs(mask, regs) \
    set_enet_regs_emir(mask, regs) \
    var irq_addr = lds {1} @base + IRQ_MASK_OFFSET \
    !stw mask, irq_addr \


/* Like a hardware ring buffer library */
#define get_hw_rx_tail(rx_tail, hw_rx) \
    var rx_tail = TMP_STORE \
    @get_hw_tail(hw_rx, 0, TMP_STORE, 1) \
    rx_tail = lds {1} TMP_STORE \

#define get_hw_tx_tail(tx_tail, hw_tx) \
    var tx_tail = TMP_STORE \
    @get_hw_tail(hw_tx, 1, TMP_STORE, 1) \
    tx_tail = lds {1} TMP_STORE \

#define set_hw_rx_tail(hw_rx) \
    @set_hw_tail(hw_rx, 0, 0, 1) \

#define set_hw_tx_tail(hw_tx) \
    @set_hw_tail(hw_tx, 1, 0, 1) \

#define get_hw_rx_head(rx_head, hw_rx) \
    var rx_head = TMP_STORE \
    @get_hw_head(hw_rx, 0, TMP_STORE, 1) \
    rx_head = lds {1} TMP_STORE \

#define get_hw_tx_head(tx_head, hw_tx) \
    var tx_head = TMP_STORE \
    @get_hw_head(hw_tx, 1, TMP_STORE, 1) \
    tx_head = lds {1} TMP_STORE \

#define set_hw_rx_head(hw_rx) \
    @set_hw_head(hw_rx, 0, 0, 1) \

#define set_hw_tx_head(hw_tx) \
    @set_hw_head(hw_tx, 1, 0, 1) \

#define get_hw_rx_meta(hw_rx, buff_io_offset, buff_len) \

#define get_hw_rx_meta(hw_rx, buff_io_offset, buff_len) \

#define set_hw_rx_meta(hw_rx, buff_io_offset, buff_len) \
    st8 buff_io_offset, TMP_LARGE \
    st8 buff_len, TMP_LARGE + WORD_SIZE \
    @set_hw_meta(hw_rx, 0, TMP_LARGE, 1) \

#define set_hw_tx_meta(hw_tx, buff_io_offset, buff_len) \
    st8 buff_io_offset, TMP_LARGE \
    st8 buff_len, TMP_LARGE + WORD_SIZE \
    @set_hw_meta(hw_tx, 1, TMP_LARGE, 1) \

#define hw_rx_ring_full(full, hw_rx) \
    var full = 0 \
    st8 full, TMP_STORE \
    @hw_ring_full(hw_rx, 0, TMP_STORE, 1) \
    full = ldw {1} TMP_STORE \

#define hw_tx_ring_full(full, hw_tx) \
    var full = 0 \
    st8 full, TMP_STORE \
    @hw_ring_full(hw_tx, 0, TMP_STORE, 1) \
    full = ldw {1} TMP_STORE \

#define hw_rx_ring_empty(empty, hw_rx) \
    var empty = 0 \
    st8 empty, TMP_STORE \
    @hw_ring_empty(hw_rx, 0, TMP_STORE, 1) \
    empty = ldw {1} TMP_STORE \

#define hw_tx_ring_empty(empty, hw_tx) \
    var empty = 0 \
    st8 empty, TMP_STORE \
    @hw_ring_empty(hw_tx, 0, TMP_STORE, 1) \
    empty = ldw {1} TMP_STORE \

#define set_hw_rx_ring_slot(hw_rx, buff_io_offset, len, stat) \

#define set_hw_tx_ring_slot(hw_tx, buff_io_offset, len, stat) \





fun main() {
    return 0;
}

/* Pancake non-main entry points on notified */
export fun pnk_rx_provide() {
    var regs = ldw @base;
    /* get corresponding hw and net queues */
    get_rx_q_handle(rx_q_handle);
    get_hw_rx(hw_rx);

    var err = 0;
    var reprocess = 1;
    while (reprocess) {
        hw_rx_ring_full(full, hw_rx);
        net_queue_empty_free(net_empty, rx_q_handle);
        while (!full && !net_empty) {
            err = net_dequeue_free(rx_q_handle, TMP_STORE);
            ASSERT(!err);
            /* TODO: use shapes? */
            var buff_io_offset = lds {1} TMP_STORE;
            var buff_len = lds {1} TMP_STORE + WORD_SIZE;

            var stat = RXD_EMPTY;
            get_hw_rx_tail(rx_tail, hw_rx);
            if (rx_tail + 1 == RX_COUNT) {
                stat = stat | WRAP;
            }
            set_hw_rx_meta(hw_rx, buff_io_offset, buff_len);
            set_hw_rx_ring_slot(hw_rx, buff_io_offset, 0, stat);
            set_hw_rx_tail(hw_rx);
        }

        hw_rx_ring_full(full, hw_rx);
        if (!full) {
            net_request_signal_free(rx_q_handle);
        } else {
            net_cancel_signal_free(rx_q_handle);
        }
        reprocess = 0;

        hw_rx_ring_full(full, hw_rx);
        net_queue_empty_free(net_empty, rx_q_handle);
        if (!net_empty && !full) {
            net_cancel_signal_free(rx_q_handle);
            reprocess = 1;
        }
    }

    hw_rx_ring_empty(empty, hw_rx);
    if (!empty) {
        set_enet_regs_rdar(RDAR_RDAR);
        get_irq_mask(irq_mask);
        if (!(irq_mask & NETIRQ_RXF)) {
            enable_irqs(IRQ_MASK);
        }
    } else {
        enable_irqs(NETIRQ_TXF | NETIRQ_EBERR);
    }
}

export fun pnk_rx_return() {
    var regs = ldw @base;
    var err = 0;
    var packets_transferred = 0;
    hw_rx_ring_empty(empty, hw_rx);
    while (empty) {
        get_hw_rx_head(rx_head, hw_rx);
        get_hw_rx_descr(d_addr, hw_rx, rx_head);
        get_descr_stat(stat, d_addr);
        if (stat & RXD_EMPTY) {
            break;
        }

        get_hw_rx_descr(hw_rx, buff_io_offset, buff_len);
        get_descr_len(len, d_addr);
        buff_len = len;
        err = net_enqueue_active(rx_q_handle, buff_io_offset, buff_len);
        ASSERT(!err);

        packets_transferred = 1;
        set_hw_rx_head(hw_rx);
    }

    net_require_signal_active(req_signal, rx_q_handle);
    if (packets_transferred && req_signal) {
        net_cancel_signal_active(rx_q_handle);
        microkit_notify(RX_CH);
    }
}

export fun pnk_tx_provide() {
    var regs = ldw @base;
    /* get corresponding hw and net queues */
    get_tx_q_handle(tx_q_handle);
    get_hw_tx(hw_tx);

    var err = 0;
    var reprocess = 1;
    while (reprocess) {
        hw_tx_ring_full(full, hw_tx);
        net_queue_empty_active(net_empty, tx_q_handle);
        while (!full && !net_empty) {
            err = net_dequeue_active(rx_q_handle, TMP_STORE);
            ASSERT(!err);
            var buff_io_offset = lds {1} TMP_STORE;
            var buff_len = lds {1} TMP_STORE + WORD_SIZE;

            var stat = TXD_READY | TXD_ADDCRC | TXD_LAST;
            get_hw_tx_tail(tx_tail, hw_tx);
            if (tx_tail + 1 == TX_COUNT) {
                stat = stat | WRAP;
            }
            set_hw_tx_meta(hw_tx, buff_io_offset, buff_len);
            set_hw_tx_ring_slot(hw_tx, buff_io_offset, buff_len, stat);
            set_hw_tx_tail(hw_tx);

            get_enet_regs_tdar(tdar, regs);
            if (!(tdar & TDAR_TDAR)) {
                set_enet_regs_tdar(TDAR_TDAR, regs);
            }
        }

        net_request_signal_active(tx_q_handle);
        reprocess = 0;

        hw_rx_ring_full(full, hw_rx);
        net_queue_empty_active(net_empty, tx_q_handle);
        if (!net_empty && !full) {
            net_cancel_signal_active(tx_q_handle);
            reprocess = 1;
        }
    }
}

export fun pnk_tx_return() {
    var regs = ldw @base;
    var err = 0;
    var enqueued = 0;
    hw_tx_ring_empty(empty, hw_tx);
    while (empty) {
        get_hw_tx_head(tx_head, hw_tx);
        get_hw_tx_descr(d_addr, hw_tx, tx_head);
        get_descr_stat(stat, d_addr);
        if (stat & TXD_READY) {
            break;
        }

        get_hw_tx_descr(hw_tx, buff_io_offset, buff_len);
        buff_len = 0;

        set_hw_head(hw_tx);

        err = net_enqueue_free(tx_q_handle, buff_io_offset, buff_len);
        ASSERT(!err);
        enqueued = 1;
    }

    net_require_signal_free(req_signal, tx_q_handle);
    if (enqueued && req_signal) {
        net_cancel_signal_free(tx_q_handle);
        microkit_notify(TX_CH);
    }
}

export fun pnk_handle_irq() {
    var regs = ldw @base;
    get_enet_regs_eir(eir, regs);
    get_irq_mask(irq_mask);
    var e = eir & irq_mask;
    
    set_enet_regs_eir(e, regs);

    while (e & irq_mask) {
        if (e & NETIRQ_TXF) {
            pnk_tx_return();
        }
        if (e & NETIRQ_RXF) {
            pnk_rx_return();
            pnk_rx_provide();
        }
        if (e & NETIRQ_EBERR) {
            /* error */
            // sddf_dprintf("ETH|ERROR: System bus/uDMA\n");
        }

        get_enet_regs_eir(eir, regs);
        get_irq_mask(irq_mask);
        e = eir & irq_mask;

        set_enet_regs_eir(e, regs);
    }
}